{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26516ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa5cc9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da69741c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceeb786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearnex import patch_sklearn\n",
    "# patch_sklearn()\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, HalvingRandomSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.utils.extmath import softmax\n",
    "\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53029543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display = 'diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4826218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f067a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b55248",
   "metadata": {},
   "outputs": [],
   "source": [
    "HASHING_N_FEATURES = 150_000\n",
    "HASHING_NGRAM_RANGE = (1, 3)\n",
    "FEATURE_SELECTION_MAX_FEATURES = 75_000\n",
    "RFE_STEP = 25_000\n",
    "TEST_DATA_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a8fab0",
   "metadata": {},
   "source": [
    "# Fake news detction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a958ab82",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "In this notebook we will gather and clean data, create a fake news classifier, compare it with two other implementations, optimize the model performance on new data and create a pipeline for easier model usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc7f6d",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "\n",
    "Introduction: [Introduction](#Introduction)\n",
    "\n",
    "Terminologies: [Terminologies](#Terminologies)\n",
    "\n",
    "Project: [Project](#Project)\n",
    "\n",
    "Testing our model on different data: [Testing our model on different data](#Testing-our-model-on-different-data)\n",
    "\n",
    "Conclusion: [Conclusion](#Conclusion)\n",
    "\n",
    "References: [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517177ee",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We consume news through several mediums throughout the day in our daily routine, but sometimes it becomes difficult to decide which one is fake and which one is authentic.\n",
    "\n",
    "Do you trust all the news you consume from online media?\n",
    "\n",
    "Every news that we consume is not real. If you listen to fake news it means you are collecting the wrong information from the world which can affect society because a person’s views or thoughts can change after consuming fake news which the user perceives to be true.\n",
    "\n",
    "Since all the news we encounter in our day-to-day life is not authentic, how do we categorize if the news is fake or real?\n",
    "\n",
    "In this notebook, we will focus on text-based news and try to build a model that will help us to identify if a piece of given news is fake or real.\n",
    "\n",
    "Before moving to the practical things let’s get aware of few terminologies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d54fa",
   "metadata": {},
   "source": [
    "## Terminologies\n",
    "\n",
    "### Fake News\n",
    "\n",
    "A sort of sensationalist reporting, counterfeit news embodies bits of information that might be lies and is, for the most part, spread through web-based media and other online media.\n",
    "\n",
    "This is regularly done to further or force certain kinds of thoughts or for false promotion of products and is frequently accomplished with political plans.\n",
    "\n",
    "Such news things may contain bogus and additionally misrepresented cases and may wind up being virtualized by calculations, and clients may wind up in a channel bubble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28ef3f",
   "metadata": {},
   "source": [
    "### Hashing Vectorizer\n",
    "\n",
    "Hashing vectorizer is a vectorizer which uses the hashing trick to find the token string name to feature integer index mapping. Conversion of text documents into matrix is done by this vectorizer where it turns the collection of documents into a sparse matrix which are holding the token occurence counts. Advantage of hashing vectorizer is: \n",
    "\n",
    "* As there is no need of storing the vocabulary dictionary in the memory, for large data sets it is very low memory scalable. As there in no state during the fit, it can be used in a streaming or parallel pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a1fd2",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The confusion matrix is a matrix used to determine the performance of the classification models for a given set of test data. It can only be determined if the true values for test data are known. The matrix itself can be easily understood, but the related terminologies may be confusing. Since it shows the errors in the model performance in the form of a matrix, hence also known as an error matrix. Some features of Confusion matrix are given below:\n",
    "\n",
    "* The confusion matrix is an $ n * n $ matrix, where $ n $ is the count of classifier classes. In our case $ n $ is equal to 2 (we have two classes: fake and real news), so the confusion matrix will be a $ 2 * 2 $ matrix\n",
    "\n",
    "* The matrix is divided into two dimensions, that are predicted values and actual values along with the total number of predictions.\n",
    "\n",
    "* Predicted values are those values, which are predicted by the model, and actual values are the true values for the given observations.\n",
    "\n",
    "* A $ 2 * 2 $ confusion matrix looks like below:\n",
    "\n",
    "![Confusion matrix](images/confusion-matrix.jpeg)\n",
    "\n",
    "Source: [https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5](https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5)\n",
    "\n",
    "\n",
    "Note: Type 1 error is called false positive and type 2 error is called false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d1563",
   "metadata": {},
   "source": [
    "## Project\n",
    "\n",
    "To get the accurately classified collection of news as real or fake we have to build a machine learning model.\n",
    "\n",
    "To deals with the detection of fake or real news, we will develop the project in python with the help of \"sklearn\", we will use \"HashingVectorizer\" in our news data which we will gather from online media.\n",
    "\n",
    "After the first step is done, we will initialize the classifier, transform and fit the model. In the end, we will calculate the performance of the model using the appropriate performance matrix/matrices. Once will calculate the performance matrices we will be able to see how well our model performs.\n",
    "\n",
    "The practical implementation of these tools is very simple and will be explained step by step in this notebook.\n",
    "\n",
    "Let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd40a2",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fcf83d",
   "metadata": {},
   "source": [
    "Let's start by reading our train dataset, which is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news = pd.read_csv('data/fake-news/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cccfd53",
   "metadata": {},
   "source": [
    "Now let's see how our data looks like by getting the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c8656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5faecc7",
   "metadata": {},
   "source": [
    "The label column tells us if a news is real or fake. 1 means fake news, 0 means real news."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd35ae",
   "metadata": {},
   "source": [
    "Now we will see the columns types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b69a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8198b1",
   "metadata": {},
   "source": [
    "After that let's see how many observations (rows) and features (columns) we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fdb5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7dc292",
   "metadata": {},
   "source": [
    "We can see that we have 20800 observations on 5 features. One of those features is id which we will set as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf6e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news = train_news.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12278f0",
   "metadata": {},
   "source": [
    "We now have 4 features which are title, author, text and label, because we set id as an index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d3ef22",
   "metadata": {},
   "source": [
    "Next thing we will do is to check if we have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563ab8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_news.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ccca3f",
   "metadata": {},
   "source": [
    "We see that we have missing values at every column except label. Since the missing values are only on text columns we can fill them with empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac87e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news = train_news.fillna(value = {\"title\": \"\", \"author\": \"\", \"text\": \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c32033",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee3e41",
   "metadata": {},
   "source": [
    "After we fixed missing values, we will see if we have any duplicated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a9f66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_news.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ed397",
   "metadata": {},
   "source": [
    "We have duplicated values. Let's drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f660705",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news = train_news.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09eaef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae0c13",
   "metadata": {},
   "source": [
    "We now have 20691 observations. The row drops are done to improve model performance. After we cleaned our data we can plot a histogram to see the distribution of news in train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d656b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.bar(range(2), [len(train_news[train_news[\"label\"] == 0]),len(train_news[train_news[\"label\"] == 1])])\n",
    "\n",
    "plt.title(\"Distribution of news in train dataset\")\n",
    "\n",
    "plt.xticks(range(2), [\"Real\", \"Fake\"])\n",
    "\n",
    "plt.xlabel(\"Type of news\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe95dc4",
   "metadata": {},
   "source": [
    "Let's see if our dtypes are normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64325dd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_news.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b45080",
   "metadata": {},
   "source": [
    "The types are normal but the columns are not ordered well. We will fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee191d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news = train_news[[\"title\", \"author\", \"text\", \"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26867175",
   "metadata": {},
   "source": [
    "Now let's clean our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803729d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news = pd.read_csv(\"data/fake-news/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca3310",
   "metadata": {},
   "source": [
    "Below we can see how our test dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3989028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeba20e",
   "metadata": {},
   "source": [
    "We can see we have no label column, which is right since this is a test dataset. The labels are in \"fake-news/test-labels.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a178d4f7",
   "metadata": {},
   "source": [
    "Let's see how many observations and features we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d61fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news.shape # We have 5200 observations on 4 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b782d",
   "metadata": {},
   "source": [
    "We will now see if we have any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589903f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ef3364",
   "metadata": {},
   "source": [
    "Now let's fix the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news = test_news.fillna(value = {\"title\": \"\", \"author\": \"\", \"text\": \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9428a8",
   "metadata": {},
   "source": [
    "Before we read our test labels we have to see if we have any duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87111ae4",
   "metadata": {},
   "source": [
    "We have no duplicated values and we can proceed to test labels preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = pd.read_csv(\"data/fake-news/test-labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789e6a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad7bbd",
   "metadata": {},
   "source": [
    "Since we dropped values from test dataset we need to remove the rows related to the removed records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff5aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[\"id\"] = test_news[\"id\"] \n",
    "\n",
    "# In this cell we set the ids of test labels to the ids of test news because it is easier to now drop rows with missing ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news[\"label\"] = test_labels[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cc1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_labels.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c9b2cf",
   "metadata": {},
   "source": [
    "Before we proceed to next section we have to merge both datasets and clean the merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_news = pd.concat([train_news, test_news])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a11e98",
   "metadata": {},
   "source": [
    "Here we will drop the `id` column since it came from test dataset and most values will be `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_news = merged_news.drop(columns = [\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428bb80",
   "metadata": {},
   "source": [
    "After this processing we will check if we have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1e9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_news.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b769d6",
   "metadata": {},
   "source": [
    "Let's plot a histogram for distribution of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da54dc1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar(range(2), [len(merged_news[merged_news[\"label\"] == 0]),len(merged_news[merged_news[\"label\"] == 1])])\n",
    "\n",
    "plt.title(\"Distribution of news in merged news dataset\")\n",
    "\n",
    "plt.xticks(range(2), [\"Real\", \"Fake\"])\n",
    "\n",
    "plt.xlabel(\"Type of news\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc580816",
   "metadata": {},
   "source": [
    "We see that there are more fake than real news. Let's check what is the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fccbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_news.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8e77c",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "In this section we will \"get to know\" our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000716be",
   "metadata": {},
   "source": [
    "Firstly, we will create a wordcloud plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1484dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(data, title):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=english_stopwords,\n",
    "        random_state = 42,\n",
    "        min_word_length = 4,\n",
    "    ).generate(str(data))\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.imshow(wordcloud.recolor(colormap= 'viridis', random_state = 42), interpolation = 'bilinear')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d0411",
   "metadata": {},
   "source": [
    "After that we will get statistics for our train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178f2ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_news.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1947a18c",
   "metadata": {},
   "source": [
    "In the four cells below we filter all titles and texts that contaion the words `sensation` and `breitbart`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7bafce",
   "metadata": {},
   "source": [
    "The first filter is for titles containing breitbart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218966e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_containing_breitbart_in_title_filter = merged_news.title.str.lower().str.contains(\"breitbart\")\n",
    "\n",
    "news_containing_breitbart_in_title = merged_news[news_containing_breitbart_in_title_filter]\n",
    "\n",
    "news_containing_breitbart_in_title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060038f",
   "metadata": {},
   "source": [
    "We can see that all 5 initial values are real. This is interesting, we can see the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433493e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_containing_breitbart_in_title.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b920d",
   "metadata": {},
   "source": [
    "We see that there are more real than fake news. Next, we will see all news whose titles contain the word sensation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62510d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_containing_sensation_in_title_filter = merged_news.title.str.lower().str.contains(\"sensation\")\n",
    "\n",
    "news_containing_sensation_in_title = merged_news[news_containing_sensation_in_title_filter]\n",
    "\n",
    "news_containing_sensation_in_title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b71f39",
   "metadata": {},
   "source": [
    "We can see that only one piece of news contains the word sensation on title."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1667fe",
   "metadata": {},
   "source": [
    "Next we will see all texts that contain sensation and breitbart. The first filter is for the breitbart word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91de5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_containing_breitbart_in_text_filter = merged_news.text.str.lower().str.contains(\"breitbart\")\n",
    "\n",
    "news_containing_breitbart_in_text = merged_news[news_containing_breitbart_in_text_filter]\n",
    "\n",
    "news_containing_breitbart_in_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_containing_breitbart_in_text.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f6fc46",
   "metadata": {},
   "source": [
    "We can see again that most of the news that contain word breitbart are real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73724885",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "news_containing_sensation_in_text_filter = merged_news.text.str.lower().str.contains(\"sensation\")\n",
    "\n",
    "news_containing_sensation_in_text = merged_news[news_containing_sensation_in_text_filter]\n",
    "\n",
    "news_containing_sensation_in_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da4439b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_containing_sensation_in_text.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5f858",
   "metadata": {},
   "source": [
    "We notice that most of the news that contain sensation in text are real. Now let's see all the news with author Breitbart news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee99ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "breitbart_news = merged_news[merged_news.author == 'Breitbart News']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8ee83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "breitbart_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f6bb0",
   "metadata": {},
   "source": [
    "We see that all 5 values shown are real. Let's get the value counts for labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ffede3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "breitbart_news.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8a15c",
   "metadata": {},
   "source": [
    "We see that all Breitbart news are real. We can say that Breitbart news is a credible source. Now, let's check Consortiumnews.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9cd9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "consortium_news = merged_news[merged_news.author == 'Consortiumnews.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "consortium_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2515d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "consortium_news.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a4f590",
   "metadata": {},
   "source": [
    "We see that most of the news from Consortiumnews.com are fake. We can say that Consortiumnews.com is not a credible source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a905591",
   "metadata": {},
   "source": [
    "After that we will plot some wordclouds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f1ab0a",
   "metadata": {},
   "source": [
    "The first wordcloud is for texts of fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news = merged_news[merged_news.label == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c761b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_wordcloud(fake_news[\"text\"], \"Words frequented in fake news texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa0b52a",
   "metadata": {},
   "source": [
    "The next wordcloud is for titles of fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(fake_news[\"title\"], \"Words frequented in fake news titles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa388fd",
   "metadata": {},
   "source": [
    "The last two wordclouds are for texts and titles of real news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news = merged_news[merged_news.label == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e3cc5",
   "metadata": {},
   "source": [
    "The first wordcloud for real news is for texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee42034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_wordcloud(real_news[\"text\"], \"Words frequented in real news texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b77fa",
   "metadata": {},
   "source": [
    "The final wordcloud is for texts of real news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df187cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(real_news[\"title\"], \"Words frequented in real news titles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f02ef",
   "metadata": {},
   "source": [
    "Now we will try to find other factors than the words that determine whether news is real or fake. Let's start by creating a dataframe which includes title length and label of test news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fee554",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_length_and_label = pd.DataFrame({\"title_len\": merged_news.title.str.len(), \"label\": merged_news.label})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd5e9a",
   "metadata": {},
   "source": [
    "After that we will see the most frequent labels for titles with length over and less 50 symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d5353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_lens_more_than_50 = title_length_and_label[title_length_and_label.title_len > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcacfdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titles_lens_more_than_50.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c6851",
   "metadata": {},
   "source": [
    "After we got the values for titles with lengths less than 50, let's see the titles, authors and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed1f5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_news.loc[titles_lens_more_than_50.index][[\"title\", \"author\", \"label\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc5547",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "titles_lens_less_than_50 = title_length_and_label[title_length_and_label.title_len < 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2358a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titles_lens_less_than_50.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9ddea",
   "metadata": {},
   "source": [
    "After we got the values for titles with lengths more than 50, let's see their titles, authors and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f347261e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_news.loc[titles_lens_less_than_50.index][[\"title\", \"author\", \"label\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_news.loc[titles_lens_less_than_50[titles_lens_less_than_50.label == 0].index].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245952c",
   "metadata": {},
   "source": [
    "We can see that the most of the titles with length over 50 symbols are real while most of the titles with length less than 50 symbols are fake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb99f95",
   "metadata": {},
   "source": [
    "After that, let's see distributions of lenghts of titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_titles_by_len = title_length_and_label.sort_values('title_len', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3541ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(grouped_titles_by_len.title_len, bins = \"fd\")\n",
    "\n",
    "plt.xlabel('Lengts of titles')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.title('Distributions of lengths of titles')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3431e900",
   "metadata": {},
   "source": [
    "After we analyzed the infromation from titles' lengths, let's analyze texts' lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length_and_label = pd.DataFrame({\"text_len\": merged_news.text.str.len(), \"label\": merged_news.label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_texts_by_len = text_length_and_label.sort_values('text_len', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40346e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(grouped_texts_by_len.text_len, bins = \"fd\")\n",
    "\n",
    "plt.xlabel('Lengts of texts')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.title('Distributions of lengths of texts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01231959",
   "metadata": {},
   "source": [
    "Now let's choose lengths that we want to filter by. In our case we will choose less than 5k symbols and between 5k and 10k symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cd515",
   "metadata": {},
   "source": [
    "Firstly, we will get all titles with lengths less than 5k and the corresponding news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13929341",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_lens_less_than_5k_filter = text_length_and_label.text_len < 5000\n",
    "\n",
    "texts_lens_less_than_5k = text_length_and_label[texts_lens_less_than_5k_filter]\n",
    "\n",
    "news_with_texts_lens_less_than_5k = merged_news.loc[texts_lens_less_than_5k.index]\n",
    "\n",
    "news_with_texts_lens_less_than_5k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30512fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_with_texts_lens_less_than_5k.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6966feff",
   "metadata": {},
   "source": [
    "We can see that the counts of real and fake news have 2k difference in counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9681a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_lens_between_5k_and_10k_filter = (text_length_and_label.text_len > 5000) & (text_length_and_label.text_len <= 10000)\n",
    "\n",
    "text_lens_between_5k_and_10k = text_length_and_label[texts_lens_between_5k_and_10k_filter]\n",
    "\n",
    "news_with_texts_lens_between_5k_and_10k = merged_news.loc[text_lens_between_5k_and_10k.index]\n",
    "\n",
    "news_with_texts_lens_between_5k_and_10k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_with_texts_lens_between_5k_and_10k.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298f8fab",
   "metadata": {},
   "source": [
    "We can see that we have more real news with lengths between 5k and 10k than fake news."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a4c7f",
   "metadata": {},
   "source": [
    "Before we proceed to the next section we will generate a report with information for columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a811d021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_news.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de34576",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05635ae6",
   "metadata": {},
   "source": [
    "Here we will work with a copy of news because we do not want to modify the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22756152",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = merged_news.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1441dd2d",
   "metadata": {},
   "source": [
    "Now we will get the columns we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def028c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = messages[[\"title\", \"text\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368a490",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0e1c1",
   "metadata": {},
   "source": [
    "Here we will lemmatize all words that are not stopwords. Stop words are a set of commonly used words in any language. For example, in English, “the”, “is” and “and”, would easily qualify as stop words. Stemming is known to be a fairly crude method of doing this. Lemmatization, on the other hand, is a tool that performs full morphological analysis to more accurately find the root, or “lemma” for a word. For instance, stemming the word \"earthquake\" will generate \"earthquak\", while lemmatizing will generate \"earthquake\". That is why we will use lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c9448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(data):\n",
    "    lemmatized_content = re.sub('[^a-zA-Z]',' ',data)\n",
    "    lemmatized_content = lemmatized_content.lower()\n",
    "    lemmatized_content = lemmatized_content.split()\n",
    "    lemmatized_content = [wordnet.lemmatize(word) for word in lemmatized_content if not word in english_stopwords]\n",
    "    lemmatized_content = ' '.join(lemmatized_content)\n",
    "    \n",
    "    return lemmatized_content\n",
    "\n",
    "corpus = messages[\"title\"].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb421e6",
   "metadata": {},
   "source": [
    "### Classifier Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71312801",
   "metadata": {},
   "source": [
    "The model will be implemented by using LinearSVC classifier. The Linear Support Vector Classifier (SVC) method applies a linear kernel function to perform classification and it performs well with a large number of samples. If we compare it with the SVC model, the LinearSVC has additional parameters such as penalty normalization which applies 'L1' or 'L2' and loss function. The kernel method can not be changed in LinearSVC, because it is based on the kernel linear method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58339d",
   "metadata": {},
   "source": [
    "#### Confusion Matrix Plot Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba3109",
   "metadata": {},
   "source": [
    "In this section we will create a confusion matrix plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    See full source and example: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Here we print the if confusion matrix is normalized\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    # Here we include text that shows confusion matrix values\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd9a0e",
   "metadata": {},
   "source": [
    "#### Model Report Function\n",
    "\n",
    "In this section we create a function that prints a classification report and plots confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15767a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_report(model, X_test, y_test):\n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    print(accuracy_score(y_test, pred) * 100)\n",
    "\n",
    "    print(classification_report(y_test, pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "\n",
    "    plot_confusion_matrix(cm, classes=['Fake News', 'Real News'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff9b9b",
   "metadata": {},
   "source": [
    "#### The Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937978c6",
   "metadata": {},
   "source": [
    "Here we will define our Hashing vectorizer. We use vectorizer to convert our text data to a feature vector. Feature vectors are used widely in machine learning because of the effectiveness and practicality of representing objects in a numerical way to help with many kinds of analyses. They are good for analysis because there are many techniques for comparing feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a6edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing = HashingVectorizer(n_features = HASHING_N_FEATURES, ngram_range=HASHING_NGRAM_RANGE, binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e82ec1",
   "metadata": {},
   "source": [
    "Now we will pass it our corpus to fit and transform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84519a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hashing.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c002428",
   "metadata": {},
   "source": [
    "#### Gathering Train and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d5ba29",
   "metadata": {},
   "source": [
    "In this section, we will gather our train and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3262724",
   "metadata": {},
   "source": [
    "Let's start by getting our labels, which is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = messages['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdaedd3",
   "metadata": {},
   "source": [
    "After that we have to split our data and labels in train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = TEST_DATA_SIZE, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906d954",
   "metadata": {},
   "source": [
    "#### Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3302bd1b",
   "metadata": {},
   "source": [
    "In this section we will create our model in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = LinearSVC(random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd16b7",
   "metadata": {},
   "source": [
    "Here we fit our model, or learning it to classify fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf6f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328779d",
   "metadata": {},
   "source": [
    "Now let's test it. First we will get its accuracy on test data. After that we will plot the confusion matrix of the model to see how many wrong values there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291a08f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_report(linear_svc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9fc7ee",
   "metadata": {},
   "source": [
    "After we tested it on test data, we will test it on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b88d1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_report(linear_svc, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc304ba7",
   "metadata": {},
   "source": [
    "### Comprasions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e25bb50",
   "metadata": {},
   "source": [
    "In this section we will compare our implementation of fake news classifier with two other implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7eb04d",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1aad75",
   "metadata": {},
   "source": [
    "The first model we will compare with ours is Multinomial NB classifier. Multinomial Naive Bayes algorithm is a probabilistic learning method that is mostly used in Natural Language Processing (NLP). The algorithm is based on the Bayes theorem and predicts the tag of a text such as a piece of email or newspaper article. It calculates the probability of each tag for a given sample and then gives the tag with the highest probability as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "multimnomial_nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa329f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "multimnomial_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c6e8a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_report(multimnomial_nb, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec7aa2",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b053cf",
   "metadata": {},
   "source": [
    "The second comprasion will be done with Logistic Regression. Logistic regression estimates the probability of an event occurring, such as voted or didn't vote, based on a given dataset of independent variables. Since the outcome is a probability, the dependent variable is bounded between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da829d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(\n",
    "    random_state = 42,\n",
    "    max_iter = 10,\n",
    "    solver = 'liblinear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dc0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240725a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_report(logistic_regression, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f23e3",
   "metadata": {},
   "source": [
    "## Testing our model on different data\n",
    "\n",
    "Here we will test our model on different data (find the data source in the References)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e3dec",
   "metadata": {},
   "source": [
    "### Processing data\n",
    "\n",
    "In this section our additional data will be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8115de",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data = pd.read_csv(\"data/fake_or_real_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efb85cd",
   "metadata": {},
   "source": [
    "We see that we have a column with wrong name, so lets rename it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9560c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data = additional_data.rename(columns = {\"Unnamed: 0\": \"id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606fd2a6",
   "metadata": {},
   "source": [
    "\n",
    "Now we will check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75671ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2519e2",
   "metadata": {},
   "source": [
    "Let's see how are our news distibuted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(2), \n",
    "        [len(additional_data[additional_data[\"label\"] == \"REAL\"]),\n",
    "         len(additional_data[additional_data[\"label\"] == \"FAKE\"])\n",
    "])\n",
    "\n",
    "plt.title(\"Distribution of news in dataset\")\n",
    "\n",
    "plt.xticks(range(2), [\"Real\", \"Fake\"])\n",
    "\n",
    "plt.xlabel(\"Type of news\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0256ef",
   "metadata": {},
   "source": [
    "### Testing the data on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44dd37c",
   "metadata": {},
   "source": [
    "In this section we will use our processed additional data to perfrom tests on our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743dbba1",
   "metadata": {},
   "source": [
    "Now we have to set our label column to numbers (0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data.label = additional_data.label.replace([\"REAL\", \"FAKE\"], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = additional_data.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bdebe0",
   "metadata": {},
   "source": [
    "Now we need to use our text lemmatization function to prepare our additional data for Hashing Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d37de",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_corpus = additional_data[\"title\"].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49dac98",
   "metadata": {},
   "source": [
    "Here we use new Hashing Vectorizer to convert our sample to feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de056cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing = HashingVectorizer(n_features = HASHING_N_FEATURES, ngram_range=HASHING_NGRAM_RANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfea6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_additional = hashing.transform(additional_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f5ddc",
   "metadata": {},
   "source": [
    "Now let's get our splitted additional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3918de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_additional_train, X_additional_test, y_additional_train, y_additional_test = train_test_split(X_additional,\n",
    "                                                                                                labels,\n",
    "                                                                                                random_state = 42,\n",
    "                                                                                                test_size = TEST_DATA_SIZE,\n",
    "                                                                                                stratify = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c0bae3",
   "metadata": {},
   "source": [
    "Let's get our prediction values, a classification report and plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = linear_svc.predict(X_additional_test)\n",
    "\n",
    "model_report(linear_svc, X_additional_test, y_additional_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab57022",
   "metadata": {},
   "source": [
    "Here we see that despite the high accuracy on train/test data the model performs not as good as it performs on train data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40ecf2",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb0150",
   "metadata": {},
   "source": [
    "The problem we encounter is called `overfitting`. It occures when a model \"learns\" the training data too well and can't generalize on new data. `Overfitting` can be detected if there is a significant difference in accuracy between train and test data or the model has very low accuracy on data from another dataset. An image will help us understand this better:\n",
    "\n",
    "![Overfitting](images/overfitting.png)\n",
    "\n",
    "_While the black line fits the data well, the green line is overfit._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8627da1",
   "metadata": {},
   "source": [
    "To prevent overfitting there are many techniques, but here we will show three of them: adding more data, tuning model hyperparameters and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254db3fa",
   "metadata": {},
   "source": [
    "### Adding more data\n",
    "\n",
    "Here we will add 4 more datasets, sources can be found in References. The first dataset we will read is the ISOT news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94056c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "isot_news_fake = pd.read_csv(\"data/additional_train_test_data/isot_news/Fake.csv\")\n",
    "isot_news_real = pd.read_csv(\"data/additional_train_test_data/isot_news/True.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042ce9c",
   "metadata": {},
   "source": [
    "Here we will set labels on both parts of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "isot_news_fake[\"label\"] = 1\n",
    "isot_news_real[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3445d145",
   "metadata": {},
   "source": [
    "Now, we will merge the both parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126d47c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "isot_news = pd.concat([isot_news_real, isot_news_fake])\n",
    "\n",
    "isot_news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093bcda7",
   "metadata": {},
   "source": [
    "Let's see what are the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "isot_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83738db7",
   "metadata": {},
   "source": [
    "We do not need the `subject` and `date` columns because they do will not bring the model information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "isot_news = isot_news[[\"title\", \"text\", \"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226973f",
   "metadata": {},
   "source": [
    "The next dataset we will read is the WELFake dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f7286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "welfake = pd.read_csv(\"data/additional_train_test_data/WELFake/WELFake_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13cfa5a",
   "metadata": {},
   "source": [
    "In the WELFake dataset we have 72134 news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a55e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "welfake.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992539b5",
   "metadata": {},
   "source": [
    "The `Unnamed: 0` is an index that we do not need because the default index is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44718811",
   "metadata": {},
   "outputs": [],
   "source": [
    "welfake = welfake.drop(columns = [\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0cd6a",
   "metadata": {},
   "source": [
    "Now we will read the Source based fake news classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d50717",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_based_fake_news = pd.read_csv(\"data/additional_train_test_data/source_based_fake_news_classification/news_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_based_fake_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862008a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_based_fake_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c16d9d7",
   "metadata": {},
   "source": [
    "We do not need some of the columns because we classify whether news is fake or real by title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_based_fake_news = source_based_fake_news[[\"title\", \"text\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_based_fake_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa81253",
   "metadata": {},
   "source": [
    "We need to convert the label column to numeric values because our model can handle only numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_based_fake_news[\"label\"] = source_based_fake_news[\"label\"].replace({\"Real\": 0, \"Fake\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a32df1",
   "metadata": {},
   "source": [
    "Now, we will read the 3rd dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed1817",
   "metadata": {},
   "source": [
    "Firstly, we will read the part with the fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cff6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_real_dataset_fake = pd.read_csv(\"data/additional_train_test_data/fake-and-real/Fake.csv\")\n",
    "fake_real_dataset_fake[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa732660",
   "metadata": {},
   "source": [
    "Here, we will see how many records are in the file for fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aff914",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_real_dataset_fake.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794353a2",
   "metadata": {},
   "source": [
    "Now, we will see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_real_dataset_fake.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b9ae72",
   "metadata": {},
   "source": [
    "Now, we will read the file with real news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c093fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_real_dataset_real = pd.read_csv(\"data/additional_train_test_data/fake-and-real/True.csv\")\n",
    "fake_real_dataset_real[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f4d6a",
   "metadata": {},
   "source": [
    "Here, we will see how many records are in the file for real news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578729d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_real_dataset_real.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b130a4f",
   "metadata": {},
   "source": [
    "Now, we will see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe28353",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_real_dataset_real.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43929c84",
   "metadata": {},
   "source": [
    "Now, we will merge the both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_real_dataset = pd.concat([fake_real_dataset_fake, fake_real_dataset_real])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433ed6b",
   "metadata": {},
   "source": [
    "Here we will see how many records in total there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8515358",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_real_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946948a0",
   "metadata": {},
   "source": [
    "Now, we will take a look at the merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fba9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_real_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd28a62",
   "metadata": {},
   "source": [
    "We do not need the `subject` and `date` columns, so we can remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1602d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_real_dataset = fake_real_dataset.drop(columns = [\"subject\", \"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb5fcbc",
   "metadata": {},
   "source": [
    "Now, we will read the last dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data = pd.read_csv(\"data/additional_train_test_data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceb4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16abd7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562ce98a",
   "metadata": {},
   "source": [
    "Here we must drop the `URLs` column and rename the other three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data = last_data.drop(columns = [\"URLs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data.columns = [\"title\", \"text\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2cc56d",
   "metadata": {},
   "source": [
    "Now the last dataset looks better. The final step is to merge all those and the initial train/test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = pd.concat([merged_news, isot_news, welfake, source_based_fake_news, fake_real_dataset, last_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42cd486",
   "metadata": {},
   "source": [
    "Now, let's check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7332002",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b147ee6b",
   "metadata": {},
   "source": [
    "We see that the `author` and `data` columns have a lot of missing values, so we can remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0598a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = news_dataset[[\"title\", \"text\", \"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eafc959",
   "metadata": {},
   "source": [
    "We see that one `label` value is missing so we can drop the row that has missing label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = news_dataset[~news_dataset.label.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5332f2f7",
   "metadata": {},
   "source": [
    "Now, we must preprocess the missing values of `title` and `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078854b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = news_dataset.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c588fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b406f",
   "metadata": {},
   "source": [
    "We have to reset index because we merged datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = news_dataset.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a6b08",
   "metadata": {},
   "source": [
    "We have no missing values and did reset index, so we can proceed to dropping duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = news_dataset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f99e4a8",
   "metadata": {},
   "source": [
    "Now we have no duplicates missing values, which means we can proceed to data transformation. Firstly, we will lemmatize all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15140d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset_corpus = news_dataset[\"title\"].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ced7e",
   "metadata": {},
   "source": [
    "Secondly, we will get labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset_labels = news_dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1631b74",
   "metadata": {},
   "source": [
    "Thirdly, we will use `HashingVectorizer` to transform our titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d95e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(n_features = HASHING_N_FEATURES, ngram_range = HASHING_NGRAM_RANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset_corpus_hashed = vectorizer.transform(news_dataset_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_more, X_test_more, y_train_more, y_test_more = train_test_split(\n",
    "    news_dataset_corpus_hashed,\n",
    "    news_dataset_labels,\n",
    "    test_size = TEST_DATA_SIZE,\n",
    "    random_state = 42,\n",
    "    stratify = news_dataset_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f92a4",
   "metadata": {},
   "source": [
    "We are done with adding more data. Now, let's go to hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1a60e",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee275e",
   "metadata": {},
   "source": [
    "A model hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data.\n",
    "\n",
    "* They are often used in processes to help estimate model parameters.\n",
    "* They are often specified by the practitioner.\n",
    "* They can often be set using heuristics.\n",
    "* They are often tuned for a given predictive modeling problem.\n",
    "\n",
    "We cannot know the best value for a model hyperparameter on a given problem. We may use rules of thumb, copy values used on other problems, or search for the best value by trial and error.\n",
    "\n",
    "When a machine learning algorithm is tuned for a specific problem, then you are tuning the hyperparameters of the model or order to discover the parameters of the model that result in the most skillful predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58419b",
   "metadata": {},
   "source": [
    "For hyperparameter tuning, we will use `HalvingRandomSearchCV` provided by `scikit-learn`. \n",
    "\n",
    "HalvingRandomSearchCV is randomized search on hyper parameters.\n",
    "\n",
    "The search strategy starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources.\n",
    "\n",
    "The candidates are sampled at random from the parameter space and the number of sampled candidates is determined by `n_candidates`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7749ee31",
   "metadata": {},
   "source": [
    "Firstly, all the parameters we try to tune will be explained.\n",
    "\n",
    "* The `penalty` parameter determines the type of penalty. `l1` penalty sets random values of training data to zero, while `l2` penalty subtracts a number `C` from the model weights.\n",
    "\n",
    "\n",
    "* The `loss` parameter determines the loss function. `hinge` loss is used for \"maximum-margin\" classification, while `squared_hinge` loss has the effect of the smoothing the surface of the error function and making it numerically easier to work with.\n",
    "\n",
    "\n",
    "* The `fit_intercept` parameter determines whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations.\n",
    "\n",
    "\n",
    "* The `class_weight` sets the parameter C of class i to ``class_weight[i]*C`` for SVC. If not given, all classes are supposed to have weight one.\n",
    "\n",
    "\n",
    "* The `C` parameter is the regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.\n",
    "\n",
    "\n",
    "* The `max_iter` parameter determines the maximum number of iterations for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73011a0d",
   "metadata": {},
   "source": [
    "Now, we can start the hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44916ae5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linsvc_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "    'fit_intercept': [True, False],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'C': [1e-4, 1e-2, 1e-1, 1, 10, 1e2, 1e4],\n",
    "    'max_iter': [1000, 1500, 2000, 2500, 3000],\n",
    "    'dual': [True, False]\n",
    "}\n",
    "\n",
    "linear_svc_tuner = HalvingRandomSearchCV(\n",
    "    LinearSVC(random_state = 42),\n",
    "    linsvc_grid,\n",
    "    verbose = 3,\n",
    "    random_state = 42,\n",
    "    error_score = 0,\n",
    "    scoring = 'f1'\n",
    ")\n",
    "\n",
    "linear_svc_tuner.fit(X_train_more, y_train_more)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275894ec",
   "metadata": {},
   "source": [
    "After the tuning finished, we will see what is the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da20baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc_tuner.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8adc6c",
   "metadata": {},
   "source": [
    "Now, let's create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc_tuned = LinearSVC(**linear_svc_tuner.best_params_, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f30d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc_tuned.fit(X_train_more, y_train_more)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5af19",
   "metadata": {},
   "source": [
    "Now, let's get a model report for additional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2adc591",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_report(linear_svc_tuned, X_additional_test, y_additional_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92091eb6",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "In the machine learning process, feature selection is used to make the process more accurate. It also increases the prediction power of the algorithms by selecting the most critical variables and eliminating the redundant and irrelevant ones. This is why feature selection is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e06fd6",
   "metadata": {},
   "source": [
    "Firstly, we need to initialize our feature selector and pass it the model which we select features for and the number of features we want to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selector = RFE(\n",
    "    LinearSVC(**linear_svc_tuner.best_params_),\n",
    "    n_features_to_select = FEATURE_SELECTION_MAX_FEATURES,\n",
    "    step = RFE_STEP,\n",
    "    verbose = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99040593",
   "metadata": {},
   "source": [
    "Then we have to learn it on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb0b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selector.fit(X_train_more, y_train_more)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7085497e",
   "metadata": {},
   "source": [
    "After we are done, let's transform train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14517afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_more_transformed = feature_selector.transform(X_train_more)\n",
    "X_test_more_transformed = feature_selector.transform(X_test_more)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2121d9a",
   "metadata": {},
   "source": [
    "Now, we will learn a new model with the same hyperparameters because we transformed the data based on the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e162209",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc_feature_selection = LinearSVC(**linear_svc_tuner.best_params_, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc_feature_selection.fit(X_train_more_transformed, y_train_more)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088713c",
   "metadata": {},
   "source": [
    "Now, let's test it on test data and unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e1738",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_report(linear_svc_feature_selection, X_test_more_transformed, y_test_more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd031f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_additional_test_transformed = feature_selector.transform(X_additional_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_report(linear_svc_feature_selection, X_additional_test_transformed, y_additional_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa6ff1",
   "metadata": {},
   "source": [
    "We see that our model performs well, which means that we are done with reducing overfitting of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf7dcb",
   "metadata": {},
   "source": [
    "## Creating pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e9b45",
   "metadata": {},
   "source": [
    "After we improved our model accuracy on unseen data we will make using it easier. Currently the code for getting a model prediction looks like that:\n",
    "\n",
    "````python\n",
    "news_titles = ['some title 1', 'some title 2', 'some title 3']\n",
    "news_titles_lemmatized = [lemmatize(title) for title in news_titles]\n",
    "\n",
    "news_titles_hashed = vectorizer.transform(news_titles_lemmatized)\n",
    "\n",
    "news_titles_feature_selection = feature_selector.transform(news_titles_hashed)\n",
    "\n",
    "predictions = linear_svc_tuned.predict(news_titles_feature_selection)\n",
    "````\n",
    "\n",
    "This code seems long, right? By creating a pipeline we can use the model in an easier way. The code below shows how will our code for getting predictions look like with a pipeline:\n",
    "\n",
    "````python\n",
    "\n",
    "news_titles = ['some title 1', 'some title 2', 'some title 3']\n",
    "\n",
    "predictions = fake_news_pipeline.predict(news_titles)\n",
    "````\n",
    "\n",
    "I am sure you will agree that using a pipeline makes using the model easier.\n",
    "\n",
    "But before we create the pipeline, we need to define two custom classes: text normalization transformer and `LinearSVC` extension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a815d03b",
   "metadata": {},
   "source": [
    "### Creating Custom Classes\n",
    "\n",
    "Firstly, we will create a text normalization class because we can't pass a function as a step in a `scikit-learn` pipeline. We have two options:\n",
    "\n",
    "\n",
    "* To invoke the `scikit-learn` class called `FunctionTransformer`, but this is going to be slow because Python is not a speedy language.\n",
    "\n",
    "\n",
    "* To create a custom `scikit-learn` class that extends the proper classes because `scikit-learn` is written in `Cython`, which is Python with C-like performance. This means that our transformer will be faster than the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d05d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Does lemmatization and stopwords removal.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.stopwords = stopwords.words(\"english\")\n",
    "    \n",
    "    def normalize(self, document):\n",
    "        \n",
    "        lemma = WordNetLemmatizer()\n",
    "        stemmed_content = re.sub('[^a-zA-Z]',' ', document)\n",
    "        stemmed_content = stemmed_content.lower()\n",
    "        stemmed_content = stemmed_content.strip()\n",
    "        stemmed_content = stemmed_content.split()\n",
    "        stemmed_content = [lemma.lemmatize(word) for word in stemmed_content if not word in self.stopwords]\n",
    "        stemmed_content = ' '.join(stemmed_content)\n",
    "\n",
    "        return stemmed_content\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        result = []\n",
    "        for document in documents:\n",
    "            result.append(self.normalize(document))\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4d346",
   "metadata": {},
   "source": [
    "The second custom class we will create is extended `LinearSVC`. The extension will add a `predict_proba` method that can calculate probabilites for classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVCwithProbabilities(LinearSVC):\n",
    "    def predict_proba(self, X):\n",
    "        d = self.decision_function(X)\n",
    "        d_2d = np.c_[-d, d]\n",
    "        return softmax(d_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed7e96",
   "metadata": {},
   "source": [
    "We are done with creating custom classes. Now we can proceed with creating the \n",
    "pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831bfc38",
   "metadata": {},
   "source": [
    "### Defining Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f68c9d",
   "metadata": {},
   "source": [
    "Here we will define a list that will contain all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4b36d",
   "metadata": {},
   "source": [
    "Firstly, we will add `TextNormalizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29694796",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps.append(('lemmatization', TextNormalizer()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825b4f4",
   "metadata": {},
   "source": [
    "The second step of the pipeline is HashingVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667bb48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps.append(('hashing', HashingVectorizer(n_features = HASHING_N_FEATURES, ngram_range = HASHING_NGRAM_RANGE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8b787",
   "metadata": {},
   "source": [
    "The third step is feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9487c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps.append(\n",
    "    ('feature_selection', \n",
    "        RFE(\n",
    "            LinearSVCwithProbabilities(**linear_svc_tuner.best_params_, random_state = 42),\n",
    "            n_features_to_select = FEATURE_SELECTION_MAX_FEATURES,\n",
    "            step = RFE_STEP,\n",
    "            verbose = 3\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca343b",
   "metadata": {},
   "source": [
    "And the fourth and final step - our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec054c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps.append(('classifier', LinearSVCwithProbabilities(**linear_svc_tuner.best_params_, random_state = 42)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6de169",
   "metadata": {},
   "source": [
    "Now, we will create the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_pipeline = Pipeline(\n",
    "    steps = steps,\n",
    "    verbose = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b860b93",
   "metadata": {},
   "source": [
    "Since the pipeline does all the transformations, we must fit it, but on raw text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc4ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(\n",
    "    news_dataset[\"title\"],\n",
    "    news_dataset[\"label\"],\n",
    "    test_size = TEST_DATA_SIZE,\n",
    "    random_state = 42,\n",
    "    stratify = news_dataset[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_additional_text_train, X_additional_text_test, y_additional_text_train, y_additional_text_test = train_test_split(\n",
    "    additional_data[\"title\"],\n",
    "    additional_data[\"label\"],\n",
    "    test_size = TEST_DATA_SIZE,\n",
    "    random_state = 42,\n",
    "    stratify = additional_data[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_pipeline.fit(X_text_train, y_text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b2109a",
   "metadata": {},
   "source": [
    "Now, when our pipeline is ready, let's test the pipeline on our additional test dataset and generate a model report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbafff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_report(fake_news_pipeline, X_additional_text_test, y_additional_text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff8eab0",
   "metadata": {},
   "source": [
    "## Creating a fake news detection app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6bd7e0",
   "metadata": {},
   "source": [
    "We created our pipeline and this is good, but we need to make our model more accessible, because if someone wants to use our model he has to install Python, install the required packages, launch this notebook and run it. That is why we will create a web app that uses the model to classify fake news. To do this we will need the `streamlit` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb961257",
   "metadata": {},
   "source": [
    "Firstly, we have to save our pipeline to a file. This is done by the `dump` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc187069",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(fake_news_pipeline, \"fake-news-detection-pipeline.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9616baf9",
   "metadata": {},
   "source": [
    "After this is done, we have to move the new file to the folder `fake-news-detection-app` because in this folder we have the web app written."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632fb342",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we gathered and cleaned our data, implemented a model, compared it with other models, tested it on different data, improved it so it can perform well on new data, created a pipeline for our model and created a fake news detection app. Here we learned a lot of new terminologies and how to implement them with code. I hope you liked and enjoyed this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177394a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Project idea: [https://www.upgrad.com/blog/data-science-project-ideas-topics-beginners/#12_Fake_News_Detection](https://www.upgrad.com/blog/data-science-project-ideas-topics-beginners/#12_Fake_News_Detection)\n",
    "\n",
    "Initial implementation: [https://www.analyticsvidhya.com/blog/2021/07/detecting-fake-news-with-natural-language-processing/#h2_3?&utm_source=coding-window-blog&source=coding-window-blog](https://www.analyticsvidhya.com/blog/2021/07/detecting-fake-news-with-natural-language-processing/#h2_3?&utm_source=coding-window-blog&source=coding-window-blog)\n",
    "\n",
    "Train/Test data source: [https://www.kaggle.com/c/fake-news/data](https://www.kaggle.com/c/fake-news/data)\n",
    "\n",
    "Additional data source: [https://www.kaggle.com/datasets/jillanisofttech/fake-or-real-news](https://www.kaggle.com/datasets/jillanisofttech/fake-or-real-news)\n",
    "\n",
    "WELFake dataset: [https://www.kaggle.com/datasets/saurabhshahane/fake-news-classification](https://www.kaggle.com/datasets/saurabhshahane/fake-news-classification)\n",
    "\n",
    "ISOT news dataset: [https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset)\n",
    "\n",
    "Source based fake news classification: [https://www.kaggle.com/datasets/ruchi798/source-based-news-classification](https://www.kaggle.com/datasets/ruchi798/source-based-news-classification)\n",
    "\n",
    "Last additional data: [https://github.com/NamrithaGirish/FakeSites/blob/main/data.csv](https://github.com/NamrithaGirish/FakeSites/blob/main/data.csv)\n",
    "\n",
    "Scikit-learn: [https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html)\n",
    "\n",
    "More information on SVMs: [https://scikit-learn.org/stable/modules/svm.html](https://scikit-learn.org/stable/modules/svm.html)\n",
    "\n",
    "Streamlit: [https://streamlit.io/](https://streamlit.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51700571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartModel\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartModel.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(TextNormalizer().transform([\"hello i am petar\"]), return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state.detach().numpy()\n",
    "last_hidden_states = last_hidden_states.reshape(last_hidden_states.shape[1], 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70faed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6eeb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components = 2, learning_rate = 'auto')\n",
    "\n",
    "res = tsne.fit_transform(last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# ax.scatter(res[:, 0], res[:, 1], res[:, 2])\n",
    "\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_zlabel('Z')\n",
    "\n",
    "plt.scatter(res[:, 0], res[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b76a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a3f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnews_url = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "nnlm50_url = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\"\n",
    "\n",
    "embedding = hub.KerasLayer(gnews_url, trainable = True, name = \"gnews-swivel-20dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82523df",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(patience = 2, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf653ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (), dtype = tf.string),\n",
    "    embedding,\n",
    "    tf.keras.layers.Dense(512, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(256, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(128, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(32, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(16, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(news[\"title\"], news[\"label\"], batch_size = 64, validation_data = (val_data[\"title\"], val_data[\"label\"]), callbacks = [early_stop], epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5494e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e93db",
   "metadata": {},
   "source": [
    "### SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a63497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier, Perceptron, PassiveAggressiveClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import joblib as jl\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe74f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"data/news_merged_v4.csv\")\n",
    "news = news.fillna(\"\")\n",
    "\n",
    "val_data = pd.read_csv(\"data/fake_or_real_news.csv\")\n",
    "val_data.label = val_data.label.replace({\"FAKE\": 1, \"REAL\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e9629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_titles(titles):\n",
    "    non_letter_removal_regex = re.compile(\"[^a-zA-Z\\s]\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # remove non-letter symbols and lower titles\n",
    "    removed_non_letter_symbols_and_lowered_titles = [non_letter_removal_regex.sub('', title.lower().strip()) for title in titles]\n",
    "\n",
    "    # do whitespace tokenization\n",
    "    tokenized_titles = [title.split(' ') for title in removed_non_letter_symbols_and_lowered_titles]\n",
    "\n",
    "    # remove stopwords from tokenized titles\n",
    "    titles_with_no_stopwords = [[word for word in title if word not in ENGLISH_STOP_WORDS] for title in tokenized_titles]\n",
    "\n",
    "    # lemmatize titles\n",
    "    lemmatized_titles = [[lemmatizer.lemmatize(word) for word in title] for title in titles_with_no_stopwords]\n",
    "\n",
    "    return [' '.join(title).replace('  ', ' ') for title in lemmatized_titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ee3cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"clean_titles\", FunctionTransformer(clean_titles)),\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(1, 3), sublinear_tf=True)),\n",
    "        \n",
    "#         (\n",
    "#             \"ensemble\",\n",
    "#             StackingClassifier(\n",
    "#                 [\n",
    "#                     (\"ridge\", RidgeClassifier(alpha = .1, random_state = 42)),\n",
    "#                     (\"logreg\", LogisticRegression(solver=\"liblinear\", random_state=42)),\n",
    "#                     (\"mnb\", MultinomialNB(alpha=1e-3)),\n",
    "#                     (\"perc\", Perceptron(random_state=42)),\n",
    "#                     (\"pa\", PassiveAggressiveClassifier(C=0.1, random_state=42)),\n",
    "#                     (\n",
    "#                         \"sgd\",\n",
    "#                         SGDClassifier(\n",
    "#                             penalty=None,\n",
    "#                             loss=\"squared_hinge\",\n",
    "#                             learning_rate=\"invscaling\",\n",
    "#                             eta0=100,\n",
    "#                             random_state=42,\n",
    "#                         ),\n",
    "#                     ),\n",
    "#                     (\"linear_svc\", LinearSVC(random_state = 42))\n",
    "#                 ],\n",
    "#                 final_estimator = GaussianNB(var_smoothing = 1)\n",
    "                \n",
    "#             ),\n",
    "#         ),\n",
    "    ],\n",
    "    verbose=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "def9152c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing clean_titles, total=  20.9s\n",
      "[Pipeline] ............. (step 2 of 2) Processing tfidf, total=  27.9s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;clean_titles&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function clean_titles at 0x000002887288C040&gt;)),\n",
       "                (&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(ngram_range=(1, 3), sublinear_tf=True))],\n",
       "         verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;clean_titles&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function clean_titles at 0x000002887288C040&gt;)),\n",
       "                (&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(ngram_range=(1, 3), sublinear_tf=True))],\n",
       "         verbose=3)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function clean_titles at 0x000002887288C040&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 3), sublinear_tf=True)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('clean_titles',\n",
       "                 FunctionTransformer(func=<function clean_titles at 0x000002887288C040>)),\n",
       "                ('tfidf',\n",
       "                 TfidfVectorizer(ngram_range=(1, 3), sublinear_tf=True))],\n",
       "         verbose=3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce213f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8850828729281768"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(val_data[\"title\"], val_data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2923810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8170544816878621"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(news[\"title\"], news[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "386baf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vect = pipe.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53941377",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_vect = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85884d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6470f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_saved = jl.load(\"models/best_pipe_v8.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b48e0635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9984214680347278"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_saved.score(val_data[\"title\"], val_data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee477ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5158897964433868"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_saved.score(news[\"title\"], news[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ef66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd2957dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, ComplementNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a3da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_titles(titles):\n",
    "    titles = [re.sub(r'[^a-zA-Z]', ' ', title) for title in titles]\n",
    "    titles = [re.sub(r'\\s+', ' ', title.lower().strip()) for title in titles]\n",
    "    tokenized_titles = [title.split(\" \") for title in titles]\n",
    "    titles_with_no_stopwords = [[word for word in title if not word in ENGLISH_STOP_WORDS] for title in tokenized_titles]\n",
    "    joined_titles = [\" \".join(title) for title in titles_with_no_stopwords]\n",
    "    \n",
    "    return joined_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f703c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(news['title'], news['label'], test_size=0.2, random_state=0, stratify=news['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv(\"../../Machine_Learning/Exam/data/fake_or_real_news.csv\").dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc4754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cleaning', FunctionTransformer(clean_titles)),\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,3), sublinear_tf = True)),\n",
    "#     ('ensemble', VotingClassifier(estimators=[\n",
    "#         ('mnb', MultinomialNB()),\n",
    "#         ('bnb', BernoulliNB()),\n",
    "#         ('cnb', ComplementNB()),\n",
    "#         ('lsvc', LinearSVC(random_state = 42)),\n",
    "#         ('lgr', LogisticRegression(solver = \"liblinear\", random_state = 42)),\n",
    "#         ('rc', RidgeClassifier(random_state = 42)),\n",
    "#         ('sgd', SGDClassifier(random_state = 42)),\n",
    "#         ('perc', Perceptron(random_state = 42)),\n",
    "#         ('pa', PassiveAggressiveClassifier(C = 1, random_state = 42, average = True))\n",
    "#     ]))\n",
    "], verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43110afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b6779",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe[-1].estimators_[-1].coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378efe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(val_data[\"title\"], val_data[\"label\"].replace({\"FAKE\": 1, \"REAL\": 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835635b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pipe.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "              'fit_intercept': [True, False],\n",
    "              'loss': ['hinge', 'squared_hinge'],\n",
    "              'average': [True, False],\n",
    "              'shuffle': [True, False],\n",
    "#               'random_state': [0, 42],\n",
    "              'class_weight': [None, 'balanced']\n",
    "              }\n",
    "\n",
    "# Create the HalvingRandomizedSearchCV object\n",
    "halving_rs = HalvingRandomSearchCV(scoring=\"f1_macro\",estimator=PassiveAggressiveClassifier(random_state = 42), param_distributions=param_grid,random_state=0,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c035757e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "halving_rs.fit(train_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ceee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "halving_rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e228af",
   "metadata": {},
   "outputs": [],
   "source": [
    "PassiveAggressiveClassifier(average = True, random_state = 42)\\\n",
    "                            .fit(train_data, y_train)\\\n",
    "                            .score(pipe.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b40d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
